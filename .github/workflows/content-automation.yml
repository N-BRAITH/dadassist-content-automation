name: Weekly Legal Content Scraping

on:
  schedule:
    - cron: '0 9 * * 1'  # Every Monday at 9 AM UTC
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape-content:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          
      - name: Run Apify scraper
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          python scripts/apify_scraper.py
          
      - name: Download and extract content
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          python scripts/content_downloader.py
          
      - name: Create run summary
        run: |
          python -c "
          import json
          import os
          from datetime import datetime
          
          # Load run info
          with open('downloads/latest_run.json', 'r') as f:
              run_info = json.load(f)
          
          # Load config for summary
          with open('config/apify_config.json', 'r') as f:
              config = json.load(f)
          
          # Create summary for email
          summary = {
              'automation_version': config.get('version', '1.0.0'),
              'config_last_updated': config.get('last_updated', 'unknown'),
              'run_date': datetime.now().isoformat(),
              'success': run_info.get('success', False),
              'articles_found': run_info.get('url_count', 0),
              'quality_articles': run_info.get('quality_articles', 0),
              'search_terms': config['search_queries'],
              'target_sites': config['target_sites'],
              'results_directory': run_info.get('results_dir', ''),
              'categories': run_info.get('categories', {}),
              'extraction_method': run_info.get('extraction_method', 'unknown'),
              'smtp_provider': config.get('email_settings', {}).get('smtp_provider', 'amazon_ses')
          }
          
          with open('run_summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print(f'Summary created: {summary[\"articles_found\"]} articles found (v{summary[\"automation_version\"]})')
          "
          
      - name: Upload content artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: extracted-articles-${{ github.run_number }}
          path: |
            downloads/
            run_summary.json
            config/apify_config.json
          retention-days: 30
          
      - name: Send notification
        env:
          NOTIFICATION_EMAIL: ${{ secrets.NOTIFICATION_EMAIL }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          SENDER_EMAIL: ${{ secrets.SENDER_EMAIL }}
          SES_USERNAME: ${{ secrets.SES_USERNAME }}
        run: |
          python scripts/notifier.py
          
      - name: Clean up
        if: always()
        run: |
          echo "Workflow completed successfully"
