name: Weekly Legal Content Scraping

permissions:
  contents: write
  actions: write

on:
  schedule:
    - cron: '0 9 * * 1'  # Every Monday at 9 AM UTC
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape-content:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          
      - name: Run Apify scraper
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          python scripts/apify_scraper.py
          
      - name: Download and extract content
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          python scripts/content_downloader.py
          
      - name: Create run summary
        run: |
          python -c "
          import json
          import os
          from datetime import datetime
          
          # Load run info
          with open('downloads/latest_run.json', 'r') as f:
              run_info = json.load(f)
          
          # Load config for summary
          with open('config/apify_config.json', 'r') as f:
              config = json.load(f)
          
          # Create summary for email
          summary = {
              'automation_version': config.get('version', '1.0.0'),
              'config_last_updated': config.get('last_updated', 'unknown'),
              'run_date': datetime.now().isoformat(),
              'success': run_info.get('success', False),
              'articles_found': run_info.get('url_count', 0),
              'quality_articles': run_info.get('quality_articles', 0),
              'search_rotation': config.get('search_rotation', {}),
              'current_search': config.get('search_rotation', {}).get('current_search', 0),
              'results_directory': run_info.get('results_dir', ''),
              'categories': run_info.get('categories', {}),
              'extraction_method': run_info.get('extraction_method', 'unknown'),
              'smtp_provider': config.get('email_settings', {}).get('smtp_provider', 'amazon_ses')
          }
          
          with open('run_summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print(f'Summary created: {summary[\"articles_found\"]} articles found (v{summary[\"automation_version\"]})')
          "
          
      - name: Upload content artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: extracted-articles-${{ github.run_number }}
          path: |
            downloads/
            run_summary.json
            config/apify_config.json
          retention-days: 30
          
      - name: Send notification
        env:
          NOTIFICATION_EMAIL: ${{ secrets.NOTIFICATION_EMAIL }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          SENDER_EMAIL: ${{ secrets.SENDER_EMAIL }}
          SES_USERNAME: ${{ secrets.SES_USERNAME }}
        run: |
          python scripts/notifier.py
          
      - name: Process Articles with Duplicate Checking
        run: |
          echo "üîç Processing scraped articles with duplicate checking..."
          
          # Load all articles from content downloader
          if [ ! -f downloads/*/q_developer_input.json ]; then
            echo "‚ùå No articles found to process"
            exit 1
          fi
          
          # Get total article count
          TOTAL_ARTICLES=$(jq '.articles | length' downloads/*/q_developer_input.json)
          echo "üìã Found $TOTAL_ARTICLES articles to check"
          
          # Loop through articles until we find a non-duplicate
          ARTICLE_GENERATED=false
          
          for i in $(seq 0 $((TOTAL_ARTICLES-1))); do
            echo "üîç Checking article $((i+1))/$TOTAL_ARTICLES..."
            
            # Extract article data
            CONTENT=$(jq -r ".articles[$i].content" downloads/*/q_developer_input.json)
            TITLE=$(jq -r ".articles[$i].title" downloads/*/q_developer_input.json)
            
            # Generate filename for duplicate check
            CLEAN_TITLE=$(echo "$TITLE" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
            FILENAME="${CLEAN_TITLE}.html"
            
            echo "   Title: $TITLE"
            echo "   Filename: $FILENAME"
            
            # Check if article already exists
            if curl -s --head "https://dadassist.com.au/posts/articles/$FILENAME" | grep -q "200 OK"; then
              echo "   ‚ö†Ô∏è Duplicate found - trying next article"
              continue
            else
              echo "   ‚úÖ Unique article found - proceeding with generation"
              
              # Set environment variables for article generation
              echo "ARTICLE_CONTENT<<EOF" >> $GITHUB_ENV
              echo "$CONTENT" >> $GITHUB_ENV
              echo "EOF" >> $GITHUB_ENV
              
              echo "ARTICLE_TITLE=$TITLE" >> $GITHUB_ENV
              echo "ARTICLE_FILENAME=$FILENAME" >> $GITHUB_ENV
              echo "SKIP_GENERATION=false" >> $GITHUB_ENV
              
              ARTICLE_GENERATED=true
              break
            fi
          done
          
          if [ "$ARTICLE_GENERATED" = false ]; then
            echo "‚ö†Ô∏è All articles are duplicates - no new content to generate"
            echo "SKIP_GENERATION=true" >> $GITHUB_ENV
          fi

      - name: Generate Article on Ubuntu Server
        if: env.SKIP_GENERATION == 'false'
        id: generate_article
        uses: appleboy/ssh-action@v0.1.5
        with:
          host: 13.239.163.33
          username: ubuntu
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            echo "ü§ñ Starting article generation from scraped content..."
            
            RESULT=$(python3 /home/ubuntu/generate_article.py \
              --content "${{ env.ARTICLE_CONTENT }}" \
              --title "${{ env.ARTICLE_TITLE }}" \
              --category "Legal Procedures")
            
            echo "$RESULT"
            
            if echo "$RESULT" | grep -q "SUCCESS:"; then
              ARTICLE_URL=$(echo "$RESULT" | grep "SUCCESS:" | sed 's/.*SUCCESS: *//')
              echo "üéâ Article generated successfully!"
              echo "üåê Live URL: $ARTICLE_URL"
              echo "ARTICLE_URL=$ARTICLE_URL" >> $GITHUB_ENV
              exit 0
            else
              echo "‚ùå Article generation failed"
              exit 1
            fi
            
      - name: Trigger Social Media Posting
        if: env.SKIP_GENERATION == 'false'
        run: |
          echo "üöÄ Triggering social media posting..."
          curl -X POST \
            -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            -H "Accept: application/vnd.github.v3+json" \
            https://api.github.com/repos/${{ github.repository }}/dispatches \
            -d '{
              "event_type": "new_articles_ready",
              "client_payload": {
                "article_url": "${{ env.ARTICLE_URL }}",
                "new_articles_count": "1"
              }
            }'
          echo "‚úÖ Triggered social media posting with URL: ${{ env.ARTICLE_URL }}"
            
      - name: Skip Duplicate Article
        if: env.SKIP_GENERATION == 'true'
        run: |
          echo "‚è≠Ô∏è Skipping article generation - duplicate detected"
          echo "üîÑ Search rotation will advance to next query for fresh content"
          
      - name: Trigger Social Media Detection
        if: env.SKIP_GENERATION == 'false' && success()
        run: |
          echo "üöÄ Triggering social media detection workflow..."
          
          curl -X POST \
            -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            -H "Accept: application/vnd.github.v3+json" \
            https://api.github.com/repos/${{ github.repository }}/dispatches \
            -d '{
              "event_type": "article_created",
              "client_payload": {
                "trigger_source": "content_automation",
                "article_generated": true
              }
            }'
          
          echo "‚úÖ Social media detection workflow triggered"
          
      - name: Clean up
        if: always()
        run: |
          echo "Workflow completed successfully"
