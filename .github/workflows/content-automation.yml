name: Weekly Legal Content Scraping

permissions:
  contents: write
  actions: write

on:
  schedule:
    - cron: '0 9 * * 1'  # Every Monday at 9 AM UTC
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape-content:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          
      - name: Run Apify scraper
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          python scripts/apify_scraper.py
          
      - name: Download and extract content
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          python scripts/content_downloader.py
          
      - name: Create run summary
        run: |
          python -c "
          import json
          import os
          from datetime import datetime
          
          # Load run info
          with open('downloads/latest_run.json', 'r') as f:
              run_info = json.load(f)
          
          # Load config for summary
          with open('config/apify_config.json', 'r') as f:
              config = json.load(f)
          
          # Create summary for email
          summary = {
              'automation_version': config.get('version', '1.0.0'),
              'config_last_updated': config.get('last_updated', 'unknown'),
              'run_date': datetime.now().isoformat(),
              'success': run_info.get('success', False),
              'articles_found': run_info.get('url_count', 0),
              'quality_articles': run_info.get('quality_articles', 0),
              'search_rotation': config.get('search_rotation', {}),
              'current_search': config.get('search_rotation', {}).get('current_search', 0),
              'results_directory': run_info.get('results_dir', ''),
              'categories': run_info.get('categories', {}),
              'extraction_method': run_info.get('extraction_method', 'unknown'),
              'smtp_provider': config.get('email_settings', {}).get('smtp_provider', 'amazon_ses')
          }
          
          with open('run_summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print(f'Summary created: {summary[\"articles_found\"]} articles found (v{summary[\"automation_version\"]})')
          "
          
      - name: Upload content artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: extracted-articles-${{ github.run_number }}
          path: |
            downloads/
            run_summary.json
            config/apify_config.json
          retention-days: 30
          
      - name: Send notification
        env:
          NOTIFICATION_EMAIL: ${{ secrets.NOTIFICATION_EMAIL }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          SENDER_EMAIL: ${{ secrets.SENDER_EMAIL }}
          SES_USERNAME: ${{ secrets.SES_USERNAME }}
        run: |
          python scripts/notifier.py
          
      - name: Extract Content for Article Generation
        run: |
          # Extract first article from scraped content
          CONTENT=$(jq -r '.articles[0].content' downloads/*/q_developer_input.json)
          TITLE=$(jq -r '.articles[0].title' downloads/*/q_developer_input.json)
          
          echo "ARTICLE_CONTENT<<EOF" >> $GITHUB_ENV
          echo "$CONTENT" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV
          echo "ARTICLE_TITLE=$TITLE" >> $GITHUB_ENV
          
          echo "üìù Extracted article: $TITLE"
          echo "üìä Content length: $(echo "$CONTENT" | wc -c) characters"

      - name: Generate Article on Ubuntu Server
        uses: appleboy/ssh-action@v0.1.5
        with:
          host: 13.239.163.33
          username: ubuntu
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            echo "ü§ñ Starting article generation from scraped content..."
            
            RESULT=$(python3 /home/ubuntu/generate_article.py \
              --content "${{ env.ARTICLE_CONTENT }}" \
              --title "${{ env.ARTICLE_TITLE }}" \
              --category "Legal Procedures")
            
            echo "$RESULT"
            
            if echo "$RESULT" | grep -q "SUCCESS:"; then
              ARTICLE_URL=$(echo "$RESULT" | grep "SUCCESS:" | cut -d':' -f2)
              echo "üéâ Article generated successfully!"
              echo "üåê Live URL: $ARTICLE_URL"
              exit 0
            else
              echo "‚ùå Article generation failed"
              exit 1
            fi
            
      - name: Trigger Social Media Detection
        if: success()
        run: |
          echo "üöÄ Triggering social media detection workflow..."
          
          curl -X POST \
            -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            -H "Accept: application/vnd.github.v3+json" \
            https://api.github.com/repos/${{ github.repository }}/dispatches \
            -d '{
              "event_type": "article_created",
              "client_payload": {
                "trigger_source": "content_automation",
                "article_generated": true
              }
            }'
          
          echo "‚úÖ Social media detection workflow triggered"
          
      - name: Clean up
        if: always()
        run: |
          echo "Workflow completed successfully"
