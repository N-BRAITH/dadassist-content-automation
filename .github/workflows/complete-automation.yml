# Complete DadAssist Automation Workflow
# Version: 3.5.0
# Last Updated: 2024-10-25
# Features: Content scraping, article generation, social media posting in one workflow
# Updated: Fixed JSON syntax error and notification system

name: Complete DadAssist Automation

permissions:
  contents: write
  actions: write

on:
  schedule:
    - cron: '0 9 * * 1'  # Every Monday at 9 AM UTC
  workflow_dispatch:  # Allow manual triggering

jobs:
  complete-automation:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Display Workflow Version
        run: |
          echo "üöÄ Complete DadAssist Automation Workflow"
          echo "üìã Version: 3.4.0"
          echo "üìÖ Last Updated: 2024-10-25"
          echo "üîß Features: Fixed JSON syntax error and notification system"
          echo "==========================================="
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          
      - name: Content Scraping with Retry Logic
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          echo "üï∑Ô∏è Starting content scraping with retry logic..."
          echo "üìÖ Timestamp: $(date)"
          
          MAX_ATTEMPTS=5
          ATTEMPT=1
          
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo "üîç Scraping attempt $ATTEMPT/$MAX_ATTEMPTS"
            
            # Run scraper
            echo "üï∑Ô∏è Running Apify scraper..."
            python scripts/apify_scraper.py
            
            # Download and extract content
            echo "üì• Starting content download..."
            python scripts/content_downloader.py
            echo "‚úÖ Content download completed"
            
            # Check for duplicates (copied from original duplicate checking step)
            echo "üîç Processing scraped articles with duplicate checking..."
            
            # Load all articles from content downloader
            if [ ! -f downloads/*/q_developer_input.json ]; then
              echo "‚ùå No articles found to process"
              ATTEMPT=$((ATTEMPT + 1))
              continue
            fi
            
            # Get total article count
            TOTAL_ARTICLES=$(jq '.articles | length' downloads/*/q_developer_input.json)
            echo "üìã Found $TOTAL_ARTICLES articles to check"
            
            # Loop through articles until we find a non-duplicate
            ARTICLE_GENERATED=false
            
            for i in $(seq 0 $((TOTAL_ARTICLES-1))); do
              echo "üîç Checking article $((i+1))/$TOTAL_ARTICLES..."
              
              # Extract article data
              TITLE=$(jq -r ".articles[$i].title" downloads/*/q_developer_input.json)
              
              # Generate filename for duplicate check (match server logic)
              CLEAN_TITLE=$(echo "$TITLE" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
              CLEAN_TITLE=$(echo "$CLEAN_TITLE" | sed 's/\([0-9]\)-\([0-9]\)/\1\2/g')
              FILENAME="${CLEAN_TITLE}.html"
              
              echo "   Title: $TITLE"
              echo "   Filename: $FILENAME"
              
              # Check if article already exists
              if curl -s --head "https://dadassist.com.au/posts/articles/$FILENAME" | grep -q "200 OK"; then
                echo "   ‚ö†Ô∏è Duplicate found - trying next article"
                continue
              else
                echo "   ‚úÖ Unique article found - proceeding with generation"
                
                # Set environment variables for article generation
                CONTENT=$(jq -r ".articles[$i].content" downloads/*/q_developer_input.json)
                echo "ARTICLE_CONTENT<<EOF" >> $GITHUB_ENV
                echo "$CONTENT" >> $GITHUB_ENV
                echo "EOF" >> $GITHUB_ENV
                echo "ARTICLE_TITLE=$TITLE" >> $GITHUB_ENV
                echo "ARTICLE_FILENAME=$FILENAME" >> $GITHUB_ENV
                echo "SKIP_GENERATION=false" >> $GITHUB_ENV
                
                ARTICLE_GENERATED=true
                break
              fi
            done
            
            if [ "$ARTICLE_GENERATED" = true ]; then
              echo "‚úÖ Found unique content on attempt $ATTEMPT!"
              break
            else
              echo "‚ö†Ô∏è Attempt $ATTEMPT: All articles were duplicates"
              echo "SKIP_GENERATION=true" >> $GITHUB_ENV
              
              if [ $ATTEMPT -lt $MAX_ATTEMPTS ]; then
                echo "üîÑ Rotating to next search query and retrying..."
                
                # Force rotate search query by updating config
                python -c "import json; config=json.load(open('config/apify_config.json')); current=config['search_rotation']['current_search']; total=len(config['search_rotation']['searches']); next_search=(current+1)%total; config['search_rotation']['current_search']=next_search; json.dump(config,open('config/apify_config.json','w'),indent=2); print(f'üîÑ Rotated search: {current+1} ‚Üí {next_search+1}')"
              else
                echo "‚ùå All $MAX_ATTEMPTS attempts failed - no unique content found"
              fi
            fi
            
            ATTEMPT=$((ATTEMPT + 1))
          done
          
          echo "‚úÖ Content scraping process completed"
          
      - name: Download and extract content
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          echo "üì• Starting content download..."
          echo "üìÖ Timestamp: $(date)"
          python scripts/content_downloader.py
          echo "‚úÖ Content download completed"
          echo "üìä Checking downloaded files..."
          ls -la downloads/ || echo "No downloads directory found"
          
      - name: Create run summary
        run: |
          echo "üìã Creating run summary..."
          echo "üìÖ Timestamp: $(date)"
          python -c "
          import json
          import os
          from datetime import datetime
          
          print('üîç Loading run info...')
          # Load run info
          with open('downloads/latest_run.json', 'r') as f:
              run_info = json.load(f)
          
          print('üîç Loading config...')
          # Load config for summary
          with open('config/apify_config.json', 'r') as f:
              config = json.load(f)
          
          print('üìä Creating summary...')
          # Create summary for email
          summary = {
              'automation_version': config.get('version', '1.0.0'),
              'config_last_updated': config.get('last_updated', 'unknown'),
              'run_date': datetime.now().isoformat(),
              'success': run_info.get('success', False),
              'articles_found': run_info.get('url_count', 0),
              'quality_articles': run_info.get('quality_articles', 0),
              'search_rotation': config.get('search_rotation', {}),
              'current_search': config.get('search_rotation', {}).get('current_search', 0),
              'results_directory': run_info.get('results_dir', ''),
              'categories': run_info.get('categories', {}),
              'extraction_method': run_info.get('extraction_method', 'unknown'),
              'smtp_provider': config.get('email_settings', {}).get('smtp_provider', 'amazon_ses')
          }
          
          with open('run_summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print(f'‚úÖ Summary created: {summary[\"articles_found\"]} articles found (v{summary[\"automation_version\"]})')
          "
          echo "‚úÖ Run summary completed"
          
      - name: Upload content artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: extracted-articles-${{ github.run_number }}
          path: |
            downloads/
            run_summary.json
            config/apify_config.json
          retention-days: 30
          
      - name: Process Articles with Duplicate Checking
        run: |
          echo "üîç Processing scraped articles with duplicate checking..."
          
          # Load all articles from content downloader
          if [ ! -f downloads/*/q_developer_input.json ]; then
            echo "‚ùå No articles found to process"
            exit 1
          fi
          
          # Get total article count
          TOTAL_ARTICLES=$(jq '.articles | length' downloads/*/q_developer_input.json)
          echo "üìã Found $TOTAL_ARTICLES articles to check"
          
          # Loop through articles until we find a non-duplicate
          ARTICLE_GENERATED=false
          
          for i in $(seq 0 $((TOTAL_ARTICLES-1))); do
            echo "üîç Checking article $((i+1))/$TOTAL_ARTICLES..."
            
            # Extract article data
            CONTENT=$(jq -r ".articles[$i].content" downloads/*/q_developer_input.json)
            TITLE=$(jq -r ".articles[$i].title" downloads/*/q_developer_input.json)
            
            # Generate filename for duplicate check (match server logic)
            CLEAN_TITLE=$(echo "$TITLE" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
            # Remove hyphens around numbers to match server behavior (50-50 becomes 5050)
            CLEAN_TITLE=$(echo "$CLEAN_TITLE" | sed 's/\([0-9]\)-\([0-9]\)/\1\2/g')
            FILENAME="${CLEAN_TITLE}.html"
            
            echo "   Title: $TITLE"
            echo "   Filename: $FILENAME"
            
            # Check if article already exists
            if curl -s --head "https://dadassist.com.au/posts/articles/$FILENAME" | grep -q "200 OK"; then
              echo "   ‚ö†Ô∏è Duplicate found - trying next article"
              continue
            else
              echo "   ‚úÖ Unique article found - proceeding with generation"
              
              # Set environment variables for article generation
              echo "ARTICLE_CONTENT<<EOF" >> $GITHUB_ENV
              echo "$CONTENT" >> $GITHUB_ENV
              echo "EOF" >> $GITHUB_ENV
              
              echo "ARTICLE_TITLE=$TITLE" >> $GITHUB_ENV
              echo "ARTICLE_FILENAME=$FILENAME" >> $GITHUB_ENV
              echo "SKIP_GENERATION=false" >> $GITHUB_ENV
              
              ARTICLE_GENERATED=true
              break
            fi
          done
          
          if [ "$ARTICLE_GENERATED" = false ]; then
            echo "‚ö†Ô∏è All articles are duplicates - no new content to generate"
            echo "SKIP_GENERATION=true" >> $GITHUB_ENV
          fi

      - name: Generate Article on Ubuntu Server
        if: env.SKIP_GENERATION == 'false'
        id: generate_article
        uses: appleboy/ssh-action@v0.1.5
        with:
          host: 13.239.163.33
          username: ubuntu
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            echo "ü§ñ Starting article generation..."
            
            # Write content to files (no quotes in variables)
            cat > /tmp/article_content.txt << 'CONTENT_EOF'
            ${{ env.ARTICLE_CONTENT }}
            CONTENT_EOF
            
            cat > /tmp/article_title.txt << 'TITLE_EOF'
            ${{ env.ARTICLE_TITLE }}
            TITLE_EOF
            
            RESULT=$(python3 /home/ubuntu/generate_article.py \
              --content "$(cat /tmp/article_content.txt)" \
              --title "$(cat /tmp/article_title.txt)" \
              --category "Legal Procedures")
            
            echo "üìã Generation result:"
            echo "$RESULT"
            
            if echo "$RESULT" | grep -q "SUCCESS:"; then
              ARTICLE_URL=$(echo "$RESULT" | grep "SUCCESS:" | sed 's/.*SUCCESS: *//')
              echo "üéâ Article generated successfully!"
              echo "üåê Live URL: $ARTICLE_URL"
              
              # Write URL to temp file for retrieval step
              echo "$ARTICLE_URL" > /tmp/generated_article_url.txt
              
              echo "‚úÖ Article generation completed successfully"
              exit 0
            else
              echo "‚ùå Article generation failed"
              exit 1
            fi
            
      - name: Retrieve Article URL
        if: env.SKIP_GENERATION == 'false'
        id: retrieve_url
        uses: appleboy/ssh-action@v0.1.5
        with:
          host: 13.239.163.33
          username: ubuntu
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            echo "üîç Retrieving generated article URL..."
            if [ -f /tmp/generated_article_url.txt ]; then
              ARTICLE_URL=$(cat /tmp/generated_article_url.txt)
              echo "üåê Retrieved URL: $ARTICLE_URL"
              echo "ARTICLE_URL=$ARTICLE_URL"
            else
              echo "‚ùå URL file not found"
              exit 1
            fi
            
      - name: Set Article URL
        if: env.SKIP_GENERATION == 'false'
        run: |
          echo "üîç Constructing URL from filename..."
          ARTICLE_URL="https://dadassist.com.au/posts/articles/${{ env.ARTICLE_FILENAME }}"
          echo "üåê Final URL: $ARTICLE_URL"
          echo "ARTICLE_URL=$ARTICLE_URL" >> $GITHUB_ENV
            
      - name: Generate Instagram Image
        if: env.SKIP_GENERATION != 'true'
        run: |
          echo "üé® Generating custom Instagram image..."
          echo "üìù Article Title: ${{ env.ARTICLE_TITLE }}"
          
          # Copy image generation script to server
          scp -i ~/.ssh/LightsailDefaultKey-ap-southeast-2.pem -o StrictHostKeyChecking=no \
            scripts/generate_instagram_image.py ubuntu@13.239.163.33:/tmp/
          
          # Generate image and capture URL
          INSTAGRAM_IMAGE_URL=$(ssh -i ~/.ssh/LightsailDefaultKey-ap-southeast-2.pem -o StrictHostKeyChecking=no \
            ubuntu@13.239.163.33 "cd /tmp && python3 generate_instagram_image.py '${{ env.ARTICLE_TITLE }}' | tail -1")
          
          echo "INSTAGRAM_IMAGE_URL=$INSTAGRAM_IMAGE_URL" >> $GITHUB_ENV
          echo "üåê Generated image URL: $INSTAGRAM_IMAGE_URL"
            
      - name: Post to Social Media
        if: env.SKIP_GENERATION == 'false'
        env:
          TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}
          TWITTER_API_SECRET: ${{ secrets.TWITTER_API_SECRET }}
          TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}
          TWITTER_ACCESS_SECRET: ${{ secrets.TWITTER_ACCESS_SECRET }}
          FACEBOOK_ACCESS_TOKEN: ${{ secrets.FACEBOOK_ACCESS_TOKEN }}
          FACEBOOK_PAGE_ID: ${{ secrets.FACEBOOK_PAGE_ID }}
          INSTAGRAM_ACCESS_TOKEN: ${{ secrets.INSTAGRAM_ACCESS_TOKEN }}
          INSTAGRAM_ACCOUNT_ID: ${{ secrets.INSTAGRAM_ACCOUNT_ID }}
        run: |
          echo "üì± Starting social media posting..."
          echo "üìÖ Timestamp: $(date)"
          echo "üåê Article URL: ${{ env.ARTICLE_URL }}"
          echo "üìù Article Title: ${{ env.ARTICLE_TITLE }}"
          echo "üìÇ Article Filename: ${{ env.ARTICLE_FILENAME }}"
          
          # Validate URL
          if [ -z "${{ env.ARTICLE_URL }}" ]; then
            echo "‚ùå ERROR: Article URL is empty!"
            exit 1
          fi
          
          echo "üîß Creating social media post data..."
          # Create social media post data
          mkdir -p social-media
          
          # Create JSON with printf to avoid YAML conflicts
          printf '{\n  "detection_date": "%s",\n  "new_articles_count": 1,\n  "new_articles": [{\n    "filename": "%s",\n    "title": "%s",\n    "description": "Expert legal advice for Australian fathers",\n    "url": "%s",\n    "discovered_date": "%s"\n  }]\n}' \
            "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" \
            "${{ env.ARTICLE_FILENAME }}" \
            "${{ env.ARTICLE_TITLE }}" \
            "${{ env.ARTICLE_URL }}" \
            "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" \
            > social-media/new_articles_found.json
          
          echo "‚úÖ Social media data file created:"
          cat social-media/new_articles_found.json
          
          echo "üöÄ Executing social media poster..."
          # Post to social media and capture results
          python scripts/social_media_poster.py --live > social_media_output.log 2>&1
          
          # Extract posting results for email notification
          echo "üîç Looking for social media results files..."
          ls -la social-media/results/ || echo "No results directory found"
          
          if ls social-media/results/posting_results_*.json 1> /dev/null 2>&1; then
            LATEST_RESULTS=$(ls -t social-media/results/posting_results_*.json | head -1)
            echo "SOCIAL_MEDIA_RESULTS_FILE=$LATEST_RESULTS" >> $GITHUB_ENV
            echo "‚úÖ Social media posting completed - results saved: $LATEST_RESULTS"
          else
            echo "‚ö†Ô∏è No social media results file found"
            echo "SOCIAL_MEDIA_RESULTS_FILE=" >> $GITHUB_ENV
          fi
          
          # Show output for debugging
          echo "üìã Social media posting output:"
          cat social_media_output.log
          
      - name: Display Skip Status Message
        if: env.SKIP_GENERATION == 'true'
        run: |
          echo "‚è≠Ô∏è Skipping article generation - duplicate detected"
          echo "üîÑ Search rotation will advance to next query for fresh content"
          
      - name: Summary
        if: always()
        run: |
          echo "üìä WORKFLOW SUMMARY"
          echo "==================="
          echo "üìÖ Completion time: $(date)"
          echo "üîß Workflow version: 3.4.0"
          
          if [ "${{ env.SKIP_GENERATION }}" = "true" ]; then
            echo "‚ö†Ô∏è RESULT: No new articles generated - all were duplicates"
            echo "üîÑ Next run will try different content sources"
          else
            echo "‚úÖ RESULT: Article generated and posted to social media"
            echo "üåê Article URL: ${{ env.ARTICLE_URL }}"
            echo "üìù Article Title: ${{ env.ARTICLE_TITLE }}"
            echo "üìÇ Article Filename: ${{ env.ARTICLE_FILENAME }}"
            echo "üì± Social media posting: Completed"
          fi
          
          echo "==================="
          echo "üéØ End-to-end automation completed"
          
      - name: Send Comprehensive Notification Email
        if: always()
        env:
          NOTIFICATION_EMAIL: ${{ secrets.NOTIFICATION_EMAIL }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          SENDER_EMAIL: ${{ secrets.SENDER_EMAIL }}
          SES_USERNAME: ${{ secrets.SES_USERNAME }}
          WORKFLOW_STATUS: ${{ job.status }}
          SKIP_GENERATION: ${{ env.SKIP_GENERATION }}
          ARTICLE_URL: ${{ env.ARTICLE_URL }}
          ARTICLE_TITLE: ${{ env.ARTICLE_TITLE }}
          ARTICLE_FILENAME: ${{ env.ARTICLE_FILENAME }}
          SOCIAL_MEDIA_RESULTS_FILE: ${{ env.SOCIAL_MEDIA_RESULTS_FILE }}
        run: |
          echo "üìß Sending comprehensive workflow notification..."
          echo "üìÖ Timestamp: $(date)"
          echo "üìß Notification email: $NOTIFICATION_EMAIL"
          echo "üîß Workflow status: $WORKFLOW_STATUS"
          echo "‚è≠Ô∏è Skip generation: $SKIP_GENERATION"
          echo "üì± Social media results file: $SOCIAL_MEDIA_RESULTS_FILE"
          
          # Create comprehensive summary for email
          if [ "$SKIP_GENERATION" = "true" ]; then
            echo "üìã Workflow Result: All articles were duplicates - no new content generated"
            echo "üîÑ Next run will try different search queries for fresh content"
          elif [ "$WORKFLOW_STATUS" = "success" ] && [ -n "$ARTICLE_URL" ]; then
            echo "üìã Workflow Result: Article generated and posted to social media"
            echo "üåê Article URL: $ARTICLE_URL"
            echo "üìù Article Title: $ARTICLE_TITLE"
            echo "üìÇ Article Filename: $ARTICLE_FILENAME"
          else
            echo "üìã Workflow Result: Workflow completed with issues"
            echo "‚ö†Ô∏è Check workflow logs for details"
          fi
          
          python scripts/notifier.py
          echo "‚úÖ Comprehensive notification sent"
          
      - name: Upload Debug Files
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-files-${{ github.run_number }}
          path: |
            debug_email_content.html
            social_media_output.log
            social-media/results/*.json
          retention-days: 7
          if-no-files-found: warn
