# Complete DadAssist Automation Workflow
# Version: 3.5.0
# Last Updated: 2024-10-25
# Features: Content scraping, article generation, social media posting in one workflow
# Updated: Fixed JSON syntax error and notification system

name: Complete DadAssist Automation

permissions:
  contents: write
  actions: write

on:
  schedule:
    - cron: '0 9 * * 1'  # Every Monday at 9 AM UTC
  workflow_dispatch:  # Allow manual triggering

jobs:
  complete-automation:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Display Workflow Version
        run: |
          echo "üöÄ Complete DadAssist Automation Workflow"
          echo "üìã Version: 3.4.0"
          echo "üìÖ Last Updated: 2024-10-25"
          echo "üîß Features: Fixed JSON syntax error and notification system"
          echo "==========================================="
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          
      - name: Content Scraping with Smart Retry Logic
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          echo "üï∑Ô∏è Starting content scraping with smart retry logic..."
          echo "üìÖ Timestamp: $(date)"
          
          MAX_ATTEMPTS=5
          ATTEMPT=1
          UNIQUE_FOUND=false
          
          while [ $ATTEMPT -le $MAX_ATTEMPTS ] && [ "$UNIQUE_FOUND" = false ]; do
            echo ""
            echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
            echo "üîÑ ATTEMPT $ATTEMPT/$MAX_ATTEMPTS"
            echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
            
            # STEP 1: Run scraper (with URL exclusion list)
            echo "üï∑Ô∏è Running Apify scraper with URL exclusion filtering..."
            python scripts/apify_scraper.py
            
            # Capture current search query for notification
            CURRENT_QUERY=$(python -c "import json; config=json.load(open('config/apify_config.json')); searches=config['search_rotation']['searches']; current=config['search_rotation']['current_search']; print(searches[current])")
            echo "CURRENT_SEARCH_QUERY=$CURRENT_QUERY" >> $GITHUB_ENV
            
            # STEP 2: Check if any new URLs were found
            NEW_URL_COUNT=$(python -c "import json; data=json.load(open('downloads/latest_run.json')); print(data.get('new_url_count', 0))")
            echo "üìä New URLs found: $NEW_URL_COUNT"
            
            if [ "$NEW_URL_COUNT" -eq 0 ]; then
              echo "‚ö†Ô∏è No new URLs found (all already in exclusion list)"
              echo "üîÑ Rotating to next search query and retrying..."
              
              # Rotate search query
              python -c "import json; config=json.load(open('config/apify_config.json')); current=config['search_rotation']['current_search']; total=len(config['search_rotation']['searches']); next_search=(current+1)%total; config['search_rotation']['current_search']=next_search; json.dump(config,open('config/apify_config.json','w'),indent=2); print(f'üîÑ Rotated search: {current+1} ‚Üí {next_search+1}')"
              
              ATTEMPT=$((ATTEMPT + 1))
              continue
            fi
            
            # STEP 3: Download content from new URLs
            echo "üì• Downloading content from $NEW_URL_COUNT new URLs..."
            python scripts/content_downloader.py
            echo "‚úÖ Content download completed"
            
            echo "üì• Downloading content from $NEW_URL_COUNT new URLs..."
            python scripts/content_downloader.py
            echo "‚úÖ Content download completed"
            
            # STEP 4: Check for duplicates (by source URL in metadata)
            echo "üîç Checking downloaded articles for duplicates..."
            
            # Load all articles from content downloader
            if [ ! -f downloads/*/q_developer_input.json ]; then
              echo "‚ùå No articles found to process"
              ATTEMPT=$((ATTEMPT + 1))
              continue
            fi
            
            # Get total article count
            TOTAL_ARTICLES=$(jq '.articles | length' downloads/*/q_developer_input.json)
            echo "üìã Found $TOTAL_ARTICLES articles to check"
            
            # Loop through articles until we find a non-duplicate
            ARTICLE_GENERATED=false
            
            for i in $(seq 0 $((TOTAL_ARTICLES-1))); do
              echo "üîç Checking article $((i+1))/$TOTAL_ARTICLES..."
              
              # Extract article data
              TITLE=$(jq -r ".articles[$i].title" downloads/*/q_developer_input.json)
              SOURCE_URL=$(jq -r ".articles[$i].url" downloads/*/q_developer_input.json)
              
              echo "   Title: $TITLE"
              echo "   Source URL: $SOURCE_URL"
              
              # Check if source URL already in metadata
              ALREADY_GENERATED=$(python -c "import json; metadata=json.load(open('config/article_metadata.json')); source_urls=[a['source_url'] for a in metadata['articles']]; print('true' if '$SOURCE_URL' in source_urls else 'false')")
              
              if [ "$ALREADY_GENERATED" = "true" ]; then
                echo "   ‚ö†Ô∏è Duplicate source URL - already generated"
                continue
              else
                echo "   ‚úÖ Unique source URL - proceeding with generation"
                
                # Set environment variables for article generation
                CONTENT=$(jq -r ".articles[$i].content" downloads/*/q_developer_input.json)
                echo "ARTICLE_CONTENT<<EOF" >> $GITHUB_ENV
                echo "$CONTENT" >> $GITHUB_ENV
                echo "EOF" >> $GITHUB_ENV
                
                echo "ARTICLE_TITLE=$TITLE" >> $GITHUB_ENV
                echo "ARTICLE_SOURCE_URL=$SOURCE_URL" >> $GITHUB_ENV
                echo "SKIP_GENERATION=false" >> $GITHUB_ENV
                
                # Save source URL for metadata update
                echo "{\"source_url\": \"$SOURCE_URL\"}" > downloads/latest_run.json
                
                ARTICLE_GENERATED=true
                UNIQUE_FOUND=true
                break
              fi
            done
            
            if [ "$ARTICLE_GENERATED" = true ]; then
              echo "‚úÖ Found unique content on attempt $ATTEMPT - exiting early!"
              break
            else
              echo "‚ö†Ô∏è Attempt $ATTEMPT: All articles were duplicates"
              
              if [ $ATTEMPT -lt $MAX_ATTEMPTS ]; then
                echo "üîÑ Rotating to next search query and retrying..."
                
                # Rotate search query
                python -c "import json; config=json.load(open('config/apify_config.json')); current=config['search_rotation']['current_search']; total=len(config['search_rotation']['searches']); next_search=(current+1)%total; config['search_rotation']['current_search']=next_search; json.dump(config,open('config/apify_config.json','w'),indent=2); print(f'üîÑ Rotated search: {current+1} ‚Üí {next_search+1}')"
              else
                echo "‚ùå All $MAX_ATTEMPTS attempts exhausted - no unique content found"
                echo "SKIP_GENERATION=true" >> $GITHUB_ENV
              fi
            fi
            
            ATTEMPT=$((ATTEMPT + 1))
          done
          
          # Final status
          if [ "$UNIQUE_FOUND" = false ]; then
            echo "SKIP_GENERATION=true" >> $GITHUB_ENV
          fi
          
      - name: Commit URL Exclusion List
        if: always()
        run: |
          echo "üíæ Committing updated files..."
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add config/scraped_urls.json
          git add config/apify_config.json
          git add config/article_metadata.json
          git diff --staged --quiet || git commit -m "Update scraped URLs, search rotation, and article metadata"
          git push || echo "‚ö†Ô∏è Nothing to commit or push failed"
          
          echo "‚úÖ Content scraping process completed"
          
      - name: Download and extract content
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          echo "üì• Starting content download..."
          echo "üìÖ Timestamp: $(date)"
          python scripts/content_downloader.py
          echo "‚úÖ Content download completed"
          echo "üìä Checking downloaded files..."
          ls -la downloads/ || echo "No downloads directory found"
          
      - name: Create run summary
        run: |
          echo "üìã Creating run summary..."
          echo "üìÖ Timestamp: $(date)"
          python -c "
          import json
          import os
          from datetime import datetime
          
          print('üîç Loading run info...')
          # Load run info
          with open('downloads/latest_run.json', 'r') as f:
              run_info = json.load(f)
          
          print('üîç Loading config...')
          # Load config for summary
          with open('config/apify_config.json', 'r') as f:
              config = json.load(f)
          
          print('üìä Creating summary...')
          # Create summary for email
          summary = {
              'automation_version': config.get('version', '1.0.0'),
              'config_last_updated': config.get('last_updated', 'unknown'),
              'run_date': datetime.now().isoformat(),
              'success': run_info.get('success', False),
              'articles_found': run_info.get('url_count', 0),
              'quality_articles': run_info.get('quality_articles', 0),
              'search_rotation': config.get('search_rotation', {}),
              'current_search': config.get('search_rotation', {}).get('current_search', 0),
              'results_directory': run_info.get('results_dir', ''),
              'categories': run_info.get('categories', {}),
              'extraction_method': run_info.get('extraction_method', 'unknown'),
              'smtp_provider': config.get('email_settings', {}).get('smtp_provider', 'amazon_ses')
          }
          
          with open('run_summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print(f'‚úÖ Summary created: {summary[\"articles_found\"]} articles found (v{summary[\"automation_version\"]})')
          "
          echo "‚úÖ Run summary completed"
          
      - name: Upload content artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: extracted-articles-${{ github.run_number }}
          path: |
            downloads/
            run_summary.json
            config/apify_config.json
          retention-days: 30
          
      - name: Process Articles with Duplicate Checking
        run: |
          echo "üîç Processing scraped articles with duplicate checking..."
          
          # Load all articles from content downloader
          if [ ! -f downloads/*/q_developer_input.json ]; then
            echo "‚ùå No articles found to process"
            exit 1
          fi
          
          # Get total article count
          TOTAL_ARTICLES=$(jq '.articles | length' downloads/*/q_developer_input.json)
          echo "üìã Found $TOTAL_ARTICLES articles to check"
          
          # Loop through articles until we find a non-duplicate
          ARTICLE_GENERATED=false
          
          for i in $(seq 0 $((TOTAL_ARTICLES-1))); do
            echo "üîç Checking article $((i+1))/$TOTAL_ARTICLES..."
            
            # Extract article data
            CONTENT=$(jq -r ".articles[$i].content" downloads/*/q_developer_input.json)
            TITLE=$(jq -r ".articles[$i].title" downloads/*/q_developer_input.json)
            
            # Generate filename for duplicate check (match server logic)
            CLEAN_TITLE=$(echo "$TITLE" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
            # Remove hyphens around numbers to match server behavior (50-50 becomes 5050)
            CLEAN_TITLE=$(echo "$CLEAN_TITLE" | sed 's/\([0-9]\)-\([0-9]\)/\1\2/g')
            FILENAME="${CLEAN_TITLE}.html"
            
            echo "   Title: $TITLE"
            echo "   Filename: $FILENAME"
            
            # Check if article already exists
            if curl -s --head "https://dadassist.com.au/posts/articles/$FILENAME" | grep -q "200 OK"; then
              echo "   ‚ö†Ô∏è Duplicate found - trying next article"
              continue
            else
              echo "   ‚úÖ Unique article found - proceeding with generation"
              
              # Set environment variables for article generation
              echo "ARTICLE_CONTENT<<EOF" >> $GITHUB_ENV
              echo "$CONTENT" >> $GITHUB_ENV
              echo "EOF" >> $GITHUB_ENV
              
              echo "ARTICLE_TITLE=$TITLE" >> $GITHUB_ENV
              echo "ARTICLE_FILENAME=$FILENAME" >> $GITHUB_ENV
              echo "SKIP_GENERATION=false" >> $GITHUB_ENV
              
              ARTICLE_GENERATED=true
              break
            fi
          done
          
          if [ "$ARTICLE_GENERATED" = false ]; then
            echo "‚ö†Ô∏è All articles are duplicates - no new content to generate"
            echo "SKIP_GENERATION=true" >> $GITHUB_ENV
          fi

      - name: Generate Article on Ubuntu Server
        if: env.SKIP_GENERATION == 'false'
        id: generate_article
        run: |
          echo "ü§ñ Preparing article generation..."
          
          # Get next article ID from metadata
          ARTICLE_ID=$(python -c "import json; m=json.load(open('config/article_metadata.json')); print(m['next_article_id'])")
          FILENAME=$(printf 'dadassist-article-%03d.html' $ARTICLE_ID)
          
          echo "üìù Article ID: $ARTICLE_ID"
          echo "üìÑ Filename: $FILENAME"
          echo "ARTICLE_FILENAME=$FILENAME" >> $GITHUB_ENV
          
          # We already know the URL!
          ARTICLE_URL="https://dadassist.com.au/posts/articles/$FILENAME"
          echo "ARTICLE_URL=$ARTICLE_URL" >> $GITHUB_ENV
          echo "üåê Will be published at: $ARTICLE_URL"
          
      - name: SSH Generate Article
        if: env.SKIP_GENERATION == 'false'
        uses: appleboy/ssh-action@v0.1.5
        with:
          host: 13.239.163.33
          username: ubuntu
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            echo "ü§ñ Starting article generation on server..."
            
            # Write content to files
            cat > /tmp/article_content.txt << 'CONTENT_EOF'
            ${{ env.ARTICLE_CONTENT }}
            CONTENT_EOF
            
            cat > /tmp/article_title.txt << 'TITLE_EOF'
            ${{ env.ARTICLE_TITLE }}
            TITLE_EOF
            
            # Generate article with provided filename
            RESULT=$(python3 /home/ubuntu/generate_article.py \
              --content "$(cat /tmp/article_content.txt)" \
              --title "$(cat /tmp/article_title.txt)" \
              --category "Legal Procedures" \
              --filename "${{ env.ARTICLE_FILENAME }}")
            
            echo "üìã Generation result:"
            echo "$RESULT"
            
            if echo "$RESULT" | grep -q "SUCCESS:"; then
              echo "üéâ Article generated successfully!"
              exit 0
            else
              echo "‚ùå Article generation failed"
              exit 1
            fi
              echo "üéâ Article generated successfully!"
              echo "üåê Live URL: $ARTICLE_URL"
              
              # Write URL to temp file for retrieval step
              echo "$ARTICLE_URL" > /tmp/generated_article_url.txt
              
              echo "‚úÖ Article generation completed successfully"
              exit 0
            else
              echo "‚ùå Article generation failed"
              exit 1
            fi
            
      - name: Retrieve Article URL
        if: env.SKIP_GENERATION == 'false'
        id: retrieve_url
        uses: appleboy/ssh-action@v0.1.5
        with:
          host: 13.239.163.33
          username: ubuntu
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            echo "üîç Retrieving generated article URL..."
            if [ -f /tmp/generated_article_url.txt ]; then
              ARTICLE_URL=$(cat /tmp/generated_article_url.txt)
              echo "üåê Retrieved URL: $ARTICLE_URL"
              echo "ARTICLE_URL=$ARTICLE_URL"
            else
              echo "‚ùå URL file not found"
              exit 1
            fi
            
      - name: Update Article Metadata
        if: env.SKIP_GENERATION == 'false'
        run: |
          echo "üíæ Updating article metadata..."
          
          # Get source URL from downloaded content
          SOURCE_URL=$(python -c "import json; data=json.load(open('downloads/latest_run.json')); print(data.get('source_url', 'unknown'))")
          
          # Update metadata file
          python -c "
import json
from datetime import datetime

# Load metadata
with open('config/article_metadata.json', 'r') as f:
    metadata = json.load(f)

# Add new article
article_id = metadata['next_article_id']
metadata['articles'].append({
    'article_id': article_id,
    'filename': '${{ env.ARTICLE_FILENAME }}',
    'title': '''${{ env.ARTICLE_TITLE }}''',
    'source_url': '$SOURCE_URL',
    'generated_date': datetime.now().isoformat(),
    'live_url': '${{ env.ARTICLE_URL }}',
    'category': 'Legal Procedures'
})

# Increment counter
metadata['next_article_id'] = article_id + 1
metadata['last_updated'] = datetime.now().isoformat()

# Save
with open('config/article_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)

print(f'‚úÖ Added article {article_id} to metadata')
"
          
          echo "‚úÖ Metadata updated"
            
      - name: Generate Instagram Image
        if: env.SKIP_GENERATION != 'true'
        uses: appleboy/ssh-action@v0.1.5
        with:
          host: 13.239.163.33
          username: ubuntu
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            echo "üé® Generating custom Instagram image..."
            echo "üìù Article Title: ${{ env.ARTICLE_TITLE }}"
            
            # Generate simple filename without timestamp
            SAFE_TITLE=$(echo "${{ env.ARTICLE_TITLE }}" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
            FILENAME="${SAFE_TITLE}.jpg"
            
            echo "üìÅ Using filename: $FILENAME"
            
            # Generate image with specific filename
            python3 /tmp/generate_instagram_image.py "${{ env.ARTICLE_TITLE }}" "$FILENAME"
            
            echo "‚úÖ Instagram image generated with filename: $FILENAME"
            
      - name: Set Instagram Image URL
        if: env.SKIP_GENERATION != 'true'
        run: |
          echo "üîç Constructing Instagram image URL..."
          # Generate the same filename pattern as above
          SAFE_TITLE=$(echo "${{ env.ARTICLE_TITLE }}" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
          FILENAME="${SAFE_TITLE}.jpg"
          INSTAGRAM_IMAGE_URL="https://dadassist.com.au/images/instagram/$FILENAME"
          echo "INSTAGRAM_IMAGE_URL=$INSTAGRAM_IMAGE_URL" >> $GITHUB_ENV
          echo "üåê Set Instagram image URL: $INSTAGRAM_IMAGE_URL"
            
      - name: Post to Social Media
        if: env.SKIP_GENERATION == 'false'
        env:
          TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}
          TWITTER_API_SECRET: ${{ secrets.TWITTER_API_SECRET }}
          TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}
          TWITTER_ACCESS_SECRET: ${{ secrets.TWITTER_ACCESS_SECRET }}
          FACEBOOK_ACCESS_TOKEN: ${{ secrets.FACEBOOK_ACCESS_TOKEN }}
          FACEBOOK_PAGE_ID: ${{ secrets.FACEBOOK_PAGE_ID }}
          INSTAGRAM_ACCESS_TOKEN: ${{ secrets.INSTAGRAM_ACCESS_TOKEN }}
          INSTAGRAM_ACCOUNT_ID: ${{ secrets.INSTAGRAM_ACCOUNT_ID }}
        run: |
          echo "üì± Starting social media posting..."
          echo "üìÖ Timestamp: $(date)"
          echo "üåê Article URL: ${{ env.ARTICLE_URL }}"
          echo "üìù Article Title: ${{ env.ARTICLE_TITLE }}"
          echo "üìÇ Article Filename: ${{ env.ARTICLE_FILENAME }}"
          
          # Validate URL
          if [ -z "${{ env.ARTICLE_URL }}" ]; then
            echo "‚ùå ERROR: Article URL is empty!"
            exit 1
          fi
          
          echo "üîß Creating social media post data..."
          # Create social media post data
          mkdir -p social-media
          
          # Create JSON with printf to avoid YAML conflicts
          printf '{\n  "detection_date": "%s",\n  "new_articles_count": 1,\n  "new_articles": [{\n    "filename": "%s",\n    "title": "%s",\n    "description": "Expert legal advice for Australian fathers",\n    "url": "%s",\n    "discovered_date": "%s"\n  }]\n}' \
            "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" \
            "${{ env.ARTICLE_FILENAME }}" \
            "${{ env.ARTICLE_TITLE }}" \
            "${{ env.ARTICLE_URL }}" \
            "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" \
            > social-media/new_articles_found.json
          
          echo "‚úÖ Social media data file created:"
          cat social-media/new_articles_found.json
          
          echo "üöÄ Executing social media poster..."
          # Post to social media and capture results
          python scripts/social_media_poster.py --live > social_media_output.log 2>&1
          
          # Extract posting results for email notification
          echo "üîç Looking for social media results files..."
          ls -la social-media/results/ || echo "No results directory found"
          
          if ls social-media/results/posting_results_*.json 1> /dev/null 2>&1; then
            LATEST_RESULTS=$(ls -t social-media/results/posting_results_*.json | head -1)
            echo "SOCIAL_MEDIA_RESULTS_FILE=$LATEST_RESULTS" >> $GITHUB_ENV
            echo "‚úÖ Social media posting completed - results saved: $LATEST_RESULTS"
          else
            echo "‚ö†Ô∏è No social media results file found"
            echo "SOCIAL_MEDIA_RESULTS_FILE=" >> $GITHUB_ENV
          fi
          
          # Show output for debugging
          echo "üìã Social media posting output:"
          cat social_media_output.log
          
      - name: Display Skip Status Message
        if: env.SKIP_GENERATION == 'true'
        run: |
          echo "‚è≠Ô∏è Skipping article generation - duplicate detected"
          echo "üîÑ Search rotation will advance to next query for fresh content"
          
      - name: Summary
        if: always()
        run: |
          echo "üìä WORKFLOW SUMMARY"
          echo "==================="
          echo "üìÖ Completion time: $(date)"
          echo "üîß Workflow version: 3.4.0"
          
          if [ "${{ env.SKIP_GENERATION }}" = "true" ]; then
            echo "‚ö†Ô∏è RESULT: No new articles generated - all were duplicates"
            echo "üîÑ Next run will try different content sources"
          else
            echo "‚úÖ RESULT: Article generated and posted to social media"
            echo "üåê Article URL: ${{ env.ARTICLE_URL }}"
            echo "üìù Article Title: ${{ env.ARTICLE_TITLE }}"
            echo "üìÇ Article Filename: ${{ env.ARTICLE_FILENAME }}"
            echo "üì± Social media posting: Completed"
          fi
          
          echo "==================="
          
          # Rotate search query for next run
          echo "üîÑ Advancing search query for next run..."
          python -c "import json; config=json.load(open('config/apify_config.json')); current=config['search_rotation']['current_search']; total=len(config['search_rotation']['searches']); next_search=(current+1)%total; config['search_rotation']['current_search']=next_search; json.dump(config,open('config/apify_config.json','w'),indent=2); print(f'üîÑ Advanced search: {current+1} ‚Üí {next_search+1}')"
          
          # Commit the change back to repo
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add config/apify_config.json
          git commit -m "Auto-advance search rotation" || true
          git push || true
          
          echo "üéØ End-to-end automation completed"
          
      - name: Send Comprehensive Notification Email
        if: always()
        env:
          NOTIFICATION_EMAIL: ${{ secrets.NOTIFICATION_EMAIL }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          SENDER_EMAIL: ${{ secrets.SENDER_EMAIL }}
          SES_USERNAME: ${{ secrets.SES_USERNAME }}
          WORKFLOW_STATUS: ${{ job.status }}
          SKIP_GENERATION: ${{ env.SKIP_GENERATION }}
          ARTICLE_URL: ${{ env.ARTICLE_URL }}
          ARTICLE_TITLE: ${{ env.ARTICLE_TITLE }}
          ARTICLE_FILENAME: ${{ env.ARTICLE_FILENAME }}
          SOCIAL_MEDIA_RESULTS_FILE: ${{ env.SOCIAL_MEDIA_RESULTS_FILE }}
        run: |
          echo "üìß Sending comprehensive workflow notification..."
          echo "üìÖ Timestamp: $(date)"
          echo "üìß Notification email: $NOTIFICATION_EMAIL"
          echo "üîß Workflow status: $WORKFLOW_STATUS"
          echo "‚è≠Ô∏è Skip generation: $SKIP_GENERATION"
          echo "üì± Social media results file: $SOCIAL_MEDIA_RESULTS_FILE"
          
          # Create comprehensive summary for email
          if [ "$SKIP_GENERATION" = "true" ]; then
            echo "üìã Workflow Result: All articles were duplicates - no new content generated"
            echo "üîÑ Next run will try different search queries for fresh content"
          elif [ "$WORKFLOW_STATUS" = "success" ] && [ -n "$ARTICLE_URL" ]; then
            echo "üìã Workflow Result: Article generated and posted to social media"
            echo "üåê Article URL: $ARTICLE_URL"
            echo "üìù Article Title: $ARTICLE_TITLE"
            echo "üìÇ Article Filename: $ARTICLE_FILENAME"
          else
            echo "üìã Workflow Result: Workflow completed with issues"
            echo "‚ö†Ô∏è Check workflow logs for details"
          fi
          
          python scripts/social_results_notifier.py
          echo "‚úÖ Comprehensive notification sent"
          
      - name: Upload Debug Files
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-files-${{ github.run_number }}
          path: |
            debug_email_content.html
            social_media_output.log
            social-media/results/*.json
          retention-days: 7
          if-no-files-found: warn
