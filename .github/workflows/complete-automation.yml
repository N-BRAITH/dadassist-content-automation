# Complete DadAssist Automation Workflow
# Version: 3.3.0
# Last Updated: 2024-10-19
# Features: Content scraping, article generation, social media posting in one workflow
# Updated: Fixed URL capture using file-based method

name: Complete DadAssist Automation

permissions:
  contents: write
  actions: write

on:
  schedule:
    - cron: '0 9 * * 1'  # Every Monday at 9 AM UTC
  workflow_dispatch:  # Allow manual triggering

jobs:
  complete-automation:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Display Workflow Version
        run: |
          echo "üöÄ Complete DadAssist Automation Workflow"
          echo "üìã Version: 3.3.0"
          echo "üìÖ Last Updated: 2024-10-19"
          echo "üîß Features: Fixed URL capture using file-based method"
          echo "==========================================="
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          
      - name: Run Apify scraper
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          echo "üï∑Ô∏è Starting Apify scraper..."
          echo "üìÖ Timestamp: $(date)"
          python scripts/apify_scraper.py
          echo "‚úÖ Apify scraper completed"
          
      - name: Download and extract content
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          echo "üì• Starting content download..."
          echo "üìÖ Timestamp: $(date)"
          python scripts/content_downloader.py
          echo "‚úÖ Content download completed"
          echo "üìä Checking downloaded files..."
          ls -la downloads/ || echo "No downloads directory found"
          
      - name: Create run summary
        run: |
          echo "üìã Creating run summary..."
          echo "üìÖ Timestamp: $(date)"
          python -c "
          import json
          import os
          from datetime import datetime
          
          print('üîç Loading run info...')
          # Load run info
          with open('downloads/latest_run.json', 'r') as f:
              run_info = json.load(f)
          
          print('üîç Loading config...')
          # Load config for summary
          with open('config/apify_config.json', 'r') as f:
              config = json.load(f)
          
          print('üìä Creating summary...')
          # Create summary for email
          summary = {
              'automation_version': config.get('version', '1.0.0'),
              'config_last_updated': config.get('last_updated', 'unknown'),
              'run_date': datetime.now().isoformat(),
              'success': run_info.get('success', False),
              'articles_found': run_info.get('url_count', 0),
              'quality_articles': run_info.get('quality_articles', 0),
              'search_rotation': config.get('search_rotation', {}),
              'current_search': config.get('search_rotation', {}).get('current_search', 0),
              'results_directory': run_info.get('results_dir', ''),
              'categories': run_info.get('categories', {}),
              'extraction_method': run_info.get('extraction_method', 'unknown'),
              'smtp_provider': config.get('email_settings', {}).get('smtp_provider', 'amazon_ses')
          }
          
          with open('run_summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print(f'‚úÖ Summary created: {summary[\"articles_found\"]} articles found (v{summary[\"automation_version\"]})')
          "
          echo "‚úÖ Run summary completed"
          
      - name: Upload content artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: extracted-articles-${{ github.run_number }}
          path: |
            downloads/
            run_summary.json
            config/apify_config.json
          retention-days: 30
          
      - name: Process Articles with Duplicate Checking
        run: |
          echo "üîç Processing scraped articles with duplicate checking..."
          
          # Load all articles from content downloader
          if [ ! -f downloads/*/q_developer_input.json ]; then
            echo "‚ùå No articles found to process"
            exit 1
          fi
          
          # Get total article count
          TOTAL_ARTICLES=$(jq '.articles | length' downloads/*/q_developer_input.json)
          echo "üìã Found $TOTAL_ARTICLES articles to check"
          
          # Loop through articles until we find a non-duplicate
          ARTICLE_GENERATED=false
          
          for i in $(seq 0 $((TOTAL_ARTICLES-1))); do
            echo "üîç Checking article $((i+1))/$TOTAL_ARTICLES..."
            
            # Extract article data
            CONTENT=$(jq -r ".articles[$i].content" downloads/*/q_developer_input.json)
            TITLE=$(jq -r ".articles[$i].title" downloads/*/q_developer_input.json)
            
            # Generate filename for duplicate check (match server logic)
            CLEAN_TITLE=$(echo "$TITLE" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/-/g' | sed 's/--*/-/g' | sed 's/^-\|-$//g')
            # Remove hyphens around numbers to match server behavior (50-50 becomes 5050)
            CLEAN_TITLE=$(echo "$CLEAN_TITLE" | sed 's/\([0-9]\)-\([0-9]\)/\1\2/g')
            FILENAME="${CLEAN_TITLE}.html"
            
            echo "   Title: $TITLE"
            echo "   Filename: $FILENAME"
            
            # Check if article already exists
            if curl -s --head "https://dadassist.com.au/posts/articles/$FILENAME" | grep -q "200 OK"; then
              echo "   ‚ö†Ô∏è Duplicate found - trying next article"
              continue
            else
              echo "   ‚úÖ Unique article found - proceeding with generation"
              
              # Set environment variables for article generation
              echo "ARTICLE_CONTENT<<EOF" >> $GITHUB_ENV
              echo "$CONTENT" >> $GITHUB_ENV
              echo "EOF" >> $GITHUB_ENV
              
              echo "ARTICLE_TITLE=$TITLE" >> $GITHUB_ENV
              echo "ARTICLE_FILENAME=$FILENAME" >> $GITHUB_ENV
              echo "SKIP_GENERATION=false" >> $GITHUB_ENV
              
              ARTICLE_GENERATED=true
              break
            fi
          done
          
          if [ "$ARTICLE_GENERATED" = false ]; then
            echo "‚ö†Ô∏è All articles are duplicates - no new content to generate"
            echo "SKIP_GENERATION=true" >> $GITHUB_ENV
          fi

      - name: Generate Article on Ubuntu Server
        if: env.SKIP_GENERATION == 'false'
        id: generate_article
        uses: appleboy/ssh-action@v0.1.5
        with:
          host: 13.239.163.33
          username: ubuntu
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            echo "ü§ñ Starting article generation from scraped content..."
            echo "üìÖ Timestamp: $(date)"
            echo "üìù Article Title: ${{ env.ARTICLE_TITLE }}"
            echo "üìÇ Category: Legal Procedures"
            echo "üîß Content length: $(echo '${{ env.ARTICLE_CONTENT }}' | wc -c) characters"
            
            RESULT=$(python3 /home/ubuntu/generate_article.py \
              --content "${{ env.ARTICLE_CONTENT }}" \
              --title "${{ env.ARTICLE_TITLE }}" \
              --category "Legal Procedures")
            
            echo "üìã Generation result:"
            echo "$RESULT"
            
            if echo "$RESULT" | grep -q "SUCCESS:"; then
              ARTICLE_URL=$(echo "$RESULT" | grep "SUCCESS:" | sed 's/.*SUCCESS: *//')
              echo "üéâ Article generated successfully!"
              echo "üåê Live URL: $ARTICLE_URL"
              echo "üìè URL length: $(echo $ARTICLE_URL | wc -c) characters"
              
              # Extract actual filename from the URL for file operations
              ACTUAL_FILENAME=$(echo "$ARTICLE_URL" | sed 's|.*/||')
              echo "üìÑ Generated HTML filename: $ACTUAL_FILENAME"
              
              # Check file size using actual filename
              if [ -f "/var/www/dadassist/posts/articles/$ACTUAL_FILENAME" ]; then
                echo "üìä HTML file size: $(wc -c < /var/www/dadassist/posts/articles/$ACTUAL_FILENAME) bytes"
              else
                echo "‚ö†Ô∏è HTML file not found at expected location"
              fi
              
              # Write URL to a temporary file for retrieval
              echo "$ARTICLE_URL" > /tmp/generated_article_url.txt
              
              echo "‚úÖ Article generation completed successfully"
              exit 0
            else
              echo "‚ùå Article generation failed"
              echo "üîç Full result output above for debugging"
              exit 1
            fi
            
      - name: Retrieve Article URL
        if: env.SKIP_GENERATION == 'false'
        id: retrieve_url
        uses: appleboy/ssh-action@v0.1.5
        with:
          host: 13.239.163.33
          username: ubuntu
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            echo "üîç Retrieving generated article URL..."
            if [ -f /tmp/generated_article_url.txt ]; then
              ARTICLE_URL=$(cat /tmp/generated_article_url.txt)
              echo "üåê Retrieved URL: $ARTICLE_URL"
              echo "ARTICLE_URL=$ARTICLE_URL"
            else
              echo "‚ùå URL file not found"
              exit 1
            fi
            
      - name: Set Article URL
        if: env.SKIP_GENERATION == 'false'
        run: |
          echo "üîç Constructing URL from filename..."
          ARTICLE_URL="https://dadassist.com.au/posts/articles/${{ env.ARTICLE_FILENAME }}"
          echo "üåê Final URL: $ARTICLE_URL"
          echo "ARTICLE_URL=$ARTICLE_URL" >> $GITHUB_ENV
            
      - name: Post to Social Media
        if: env.SKIP_GENERATION == 'false'
        env:
          TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}
          TWITTER_API_SECRET: ${{ secrets.TWITTER_API_SECRET }}
          TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}
          TWITTER_ACCESS_SECRET: ${{ secrets.TWITTER_ACCESS_SECRET }}
          FACEBOOK_ACCESS_TOKEN: ${{ secrets.FACEBOOK_ACCESS_TOKEN }}
          FACEBOOK_PAGE_ID: ${{ secrets.FACEBOOK_PAGE_ID }}
          INSTAGRAM_ACCESS_TOKEN: ${{ secrets.INSTAGRAM_ACCESS_TOKEN }}
          INSTAGRAM_ACCOUNT_ID: ${{ secrets.INSTAGRAM_ACCOUNT_ID }}
        run: |
          echo "üì± Starting social media posting..."
          echo "üìÖ Timestamp: $(date)"
          echo "üåê Article URL: ${{ env.ARTICLE_URL }}"
          echo "üìù Article Title: ${{ env.ARTICLE_TITLE }}"
          echo "üìÇ Article Filename: ${{ env.ARTICLE_FILENAME }}"
          
          # Validate URL
          if [ -z "${{ env.ARTICLE_URL }}" ]; then
            echo "‚ùå ERROR: Article URL is empty!"
            exit 1
          fi
          
          echo "üîß Creating social media post data..."
          # Create social media post data with proper escaping
          mkdir -p social-media
          
          # Escape special characters in title
          ESCAPED_TITLE=$(echo "${{ env.ARTICLE_TITLE }}" | sed 's/"/\\"/g' | sed "s/'/\\'/g")
          
          cat > social-media/new_articles_found.json << EOF
{
  "detection_date": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
  "new_articles_count": 1,
  "new_articles": [{
    "filename": "${{ env.ARTICLE_FILENAME }}",
    "title": "${ESCAPED_TITLE}",
    "description": "Expert legal advice for Australian fathers",
    "url": "${{ env.ARTICLE_URL }}",
    "discovered_date": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  }]
}
EOF
          
          echo "‚úÖ Social media data file created:"
          cat social-media/new_articles_found.json
          
          echo "üöÄ Executing social media poster..."
          # Post to social media
          python scripts/social_media_poster.py --live
          echo "‚úÖ Social media posting completed"
          
      - name: Display Skip Status Message
        if: env.SKIP_GENERATION == 'true'
        run: |
          echo "‚è≠Ô∏è Skipping article generation - duplicate detected"
          echo "üîÑ Search rotation will advance to next query for fresh content"
          
      - name: Summary
        if: always()
        run: |
          echo "üìä WORKFLOW SUMMARY"
          echo "==================="
          echo "üìÖ Completion time: $(date)"
          echo "üîß Workflow version: 3.3.0"
          
          if [ "${{ env.SKIP_GENERATION }}" = "true" ]; then
            echo "‚ö†Ô∏è RESULT: No new articles generated - all were duplicates"
            echo "üîÑ Next run will try different content sources"
          else
            echo "‚úÖ RESULT: Article generated and posted to social media"
            echo "üåê Article URL: ${{ env.ARTICLE_URL }}"
            echo "üìù Article Title: ${{ env.ARTICLE_TITLE }}"
            echo "üìÇ Article Filename: ${{ env.ARTICLE_FILENAME }}"
            echo "üì± Social media posting: Completed"
          fi
          
          echo "==================="
          echo "üéØ End-to-end automation completed"
          
      - name: Send Comprehensive Notification Email
        if: always()
        env:
          NOTIFICATION_EMAIL: ${{ secrets.NOTIFICATION_EMAIL }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          SENDER_EMAIL: ${{ secrets.SENDER_EMAIL }}
          SES_USERNAME: ${{ secrets.SES_USERNAME }}
          WORKFLOW_STATUS: ${{ job.status }}
          SKIP_GENERATION: ${{ env.SKIP_GENERATION }}
          ARTICLE_URL: ${{ env.ARTICLE_URL }}
          ARTICLE_TITLE: ${{ env.ARTICLE_TITLE }}
          ARTICLE_FILENAME: ${{ env.ARTICLE_FILENAME }}
        run: |
          echo "üìß Sending comprehensive workflow notification..."
          echo "üìÖ Timestamp: $(date)"
          echo "üìß Notification email: $NOTIFICATION_EMAIL"
          echo "üîß Workflow status: $WORKFLOW_STATUS"
          echo "‚è≠Ô∏è Skip generation: $SKIP_GENERATION"
          
          # Create comprehensive summary for email
          if [ "$SKIP_GENERATION" = "true" ]; then
            echo "üìã Workflow Result: All articles were duplicates - no new content generated"
            echo "üîÑ Next run will try different search queries for fresh content"
          elif [ "$WORKFLOW_STATUS" = "success" ] && [ -n "$ARTICLE_URL" ]; then
            echo "üìã Workflow Result: Article generated and posted to social media"
            echo "üåê Article URL: $ARTICLE_URL"
            echo "üìù Article Title: $ARTICLE_TITLE"
            echo "üìÇ Article Filename: $ARTICLE_FILENAME"
          else
            echo "üìã Workflow Result: Workflow completed with issues"
            echo "‚ö†Ô∏è Check workflow logs for details"
          fi
          
          python scripts/notifier.py
          echo "‚úÖ Comprehensive notification sent"
